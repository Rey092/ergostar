# type: ignore
"""

Revision ID: 53fa7ca7a5a1
Revises: 4990e3f8d8b8
Create Date: 2024-07-06 07:22:30.441280+00:00

"""
from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import sqlalchemy as sa
from alembic import op
from advanced_alchemy.types import EncryptedString, EncryptedText, GUID, ORA_JSONB, DateTimeUTC
from sqlalchemy import Text  # noqa: F401
from sqlalchemy.dialects import postgresql
if TYPE_CHECKING:
    from collections.abc import Sequence

__all__ = ["downgrade", "upgrade", "schema_upgrades", "schema_downgrades", "data_upgrades", "data_downgrades"]

sa.GUID = GUID
sa.DateTimeUTC = DateTimeUTC
sa.ORA_JSONB = ORA_JSONB
sa.EncryptedString = EncryptedString
sa.EncryptedText = EncryptedText

# revision identifiers, used by Alembic.
revision = '53fa7ca7a5a1'
down_revision = '4990e3f8d8b8'
branch_labels = None
depends_on = None


def upgrade() -> None:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UserWarning)
        with op.get_context().autocommit_block():
            schema_upgrades()
            data_upgrades()

def downgrade() -> None:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UserWarning)
        with op.get_context().autocommit_block():
            data_downgrades()
            schema_downgrades()

def schema_upgrades() -> None:
    """schema upgrade migrations go here."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('Futures')
    with op.batch_alter_table('MTM_Futures', schema=None) as batch_op:
        batch_op.drop_index('ix_MTM_Futures_future_id')
        batch_op.drop_index('ix_MTM_Futures_shop_id')

    op.drop_table('MTM_Futures')
    op.drop_table('Shop')
    # ### end Alembic commands ###

def schema_downgrades() -> None:
    """schema downgrade migrations go here."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('Shop',
    sa.Column('id', sa.BIGINT(), server_default=sa.text('nextval(\'"Shop_id_seq"\'::regclass)'), autoincrement=True, nullable=False),
    sa.Column('name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_Shop')
    )
    op.create_table('MTM_Futures',
    sa.Column('id', sa.BIGINT(), server_default=sa.text('nextval(\'"MTM_Futures_id_seq"\'::regclass)'), autoincrement=True, nullable=False),
    sa.Column('shop_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('future_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False),
    sa.CheckConstraint('shop_id IS NOT NULL AND future_id IS NOT NULL', name='ck_MTM_Futures_shop_future_check'),
    sa.PrimaryKeyConstraint('id', name='pk_MTM_Futures')
    )
    with op.batch_alter_table('MTM_Futures', schema=None) as batch_op:
        batch_op.create_index('ix_MTM_Futures_shop_id', ['shop_id'], unique=False)
        batch_op.create_index('ix_MTM_Futures_future_id', ['future_id'], unique=False)

    op.create_table('Futures',
    sa.Column('id', sa.BIGINT(), server_default=sa.text('nextval(\'"Futures_id_seq"\'::regclass)'), autoincrement=True, nullable=False),
    sa.Column('name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False),
    sa.PrimaryKeyConstraint('id', name='pk_Futures')
    )
    # ### end Alembic commands ###

def data_upgrades() -> None:
    """Add any optional data upgrade migrations here!"""

def data_downgrades() -> None:
    """Add any optional data downgrade migrations here!"""
